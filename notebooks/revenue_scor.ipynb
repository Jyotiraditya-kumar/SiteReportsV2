{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T08:43:21.106476977Z",
     "start_time": "2024-02-06T08:43:11.231052241Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyforest import * \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import math\n",
    "from ast import Dict, Tuple\n",
    "from typing import Any\n",
    "import numpy as np\n",
    "from scipy.spatial import Voronoi\n",
    "from shapely.geometry import Polygon\n",
    "from math import radians, cos, sin, asin, sqrt, atan2, degrees\n",
    "import alphashape\n",
    "from pyquadkey2 import quadkey\n",
    "import itertools \n",
    "import math \n",
    "from shapely import wkt\n",
    "import shapely as geom\n",
    "\n",
    "def split_large_polygon_by_density(polygon, points, base_polygon_area=0):\n",
    "    if type(polygon) == str:\n",
    "        polygon = wkt.loads(polygon)\n",
    "    try:\n",
    "        from sklearn.cluster import KMeans\n",
    "        polygon_area = polygon.area\n",
    "        if polygon_area < base_polygon_area:\n",
    "            return [polygon]\n",
    "        else:\n",
    "            num_splits = min( int(polygon_area / base_polygon_area), 4 )\n",
    "            kmeans = KMeans(n_clusters=num_splits)\n",
    "            kmeans.fit(points)\n",
    "\n",
    "            cluster_labels = kmeans.labels_\n",
    "            cluster_centers = kmeans.cluster_centers_\n",
    "            points_by_poly = []\n",
    "            for i in np.unique(cluster_labels):\n",
    "                points_by_poly.append(points[cluster_labels == i])\n",
    "            \n",
    "            polygons = []\n",
    "            for points in points_by_poly:\n",
    "                polygon = geom.MultiPoint([geom.Point(i[0], i[1]) for i in points]).convex_hull.simplify(0.001)\n",
    "                polygons.append(polygon)\n",
    "            \n",
    "            return polygons\n",
    "    except:\n",
    "        return [polygon]\n",
    "\n",
    "def geom_area_in_sqkm(geom):\n",
    "    import contextlib\n",
    "    import io\n",
    "    import sys\n",
    "    null_io = io.StringIO()\n",
    "    with contextlib.redirect_stdout(null_io):\n",
    "        if type(geom) == str:\n",
    "            geom = wkt.loads(geom)\n",
    "        from pyproj import Geod\n",
    "        geod = Geod(ellps=\"WGS84\")\n",
    "        area = geod.geometry_area_perimeter(geom)[0] / 10**6\n",
    "    return -area if area < 0 else area\n",
    "\n",
    "def get_street_name_locality(lat,lng,mapbox_key='pk.eyJ1IjoibGFpcjA4MjYiLCJhIjoiY2tkcGoxcnRzMDZvODJxbXk0MWhlcWN2aSJ9.5-yjt_SUq4w5JII7CvD4cA'):\n",
    "    import requests\n",
    "    BASE_URL = f\"https://api.mapbox.com/search/geocode/v6/reverse?longitude={lng}&latitude={lat}&access_token={mapbox_key}\"\n",
    "    try:\n",
    "        response = requests.get(BASE_URL)\n",
    "    except:\n",
    "        return \"\", None\n",
    "    data = response.json()\n",
    "    if len(data['features']) > 0:\n",
    "        data_1 = data['features'][0]\n",
    "        #print(data_1['properties']['context'])\n",
    "        if 'street' in data_1['properties']['context'].keys():\n",
    "            street_name = data_1['properties']['context']['street']['name']\n",
    "        else:\n",
    "            street_name = \"\"\n",
    "        \n",
    "        if 'neighborhood' in data_1['properties']['context'].keys():\n",
    "            neighbourhood = \", \" + data_1['properties']['context']['neighborhood']['name']\n",
    "        else:\n",
    "            neighbourhood = \"\"\n",
    "\n",
    "        if 'locality' in data_1['properties']['context'].keys():\n",
    "            locality = data_1['properties']['context']['locality']['name']\n",
    "        else:\n",
    "            locality = None\n",
    "        return street_name + neighbourhood, locality\n",
    "    else:\n",
    "        return \"\", None\n",
    "\n",
    "\n",
    "def get_relative_direction(x1, y1, x2, y2):\n",
    "    # Calculate the angle between the two points in radians\n",
    "    angle = math.atan2(y2 - y1, x2 - x1)\n",
    "\n",
    "    # Convert the angle from radians to degrees\n",
    "    angle_degrees = math.degrees(angle)\n",
    "\n",
    "    # Convert the angle to a positive value between 0 and 360 degrees\n",
    "    angle_degrees = (angle_degrees + 360) % 360 \n",
    "    directions = [\"North\", \"East\", \"South\",  \"West\"]\n",
    "\n",
    "    # Calculate the index of the direction based on the angle\n",
    "    index = round(angle_degrees / 45) % 4\n",
    "\n",
    "    # Return the relative direction\n",
    "    return directions[index]\n",
    "    \n",
    "\n",
    "def get_isochrone(lat,lng,travel_mode,cost_type,cost, key=\"pk.eyJ1IjoiYW1hcmRlZXA3NjI2IiwiYSI6ImNsMDlxamJyYTBmdjEzZHF2N2pvY2VjcXYifQ.nzUPgDHBn-eNwDk2T3_jBw\"):\n",
    "    import requests, logging, traceback\n",
    "    MAPBOX_ACCESS_TOKEN = key\n",
    "    try:\n",
    "        if cost_type == \"time\":\n",
    "            cost_type = \"contours_minutes\"\n",
    "        elif cost_type == \"distance\":\n",
    "            cost_type = \"contours_meters\"\n",
    "        else:\n",
    "            raise Exception(\"cost_type must be either time or distance\")\n",
    "\n",
    "        if travel_mode not in [\"driving\", \"walking\", \"cycling\" ] :\n",
    "            raise Exception(\"travel_mode must be either driving or walking or cycling\")\n",
    "\n",
    "        mapbox_url = f'https://api.mapbox.com/isochrone/v1/mapbox/{travel_mode}/{lng}%2C{lat}?{cost_type}={cost}&polygons=true&denoise=1&access_token={MAPBOX_ACCESS_TOKEN}'\n",
    "        logging.debug(mapbox_url)\n",
    "        response = requests.get(mapbox_url)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            #logging.debug(data)\n",
    "            data = data[\"features\"][0][\"geometry\"][\"coordinates\"][0]\n",
    "        else:\n",
    "            raise Exception(\"Mapbox API call failed with message: \" + response.text)\n",
    "        \n",
    "        isochrone_polygon = []\n",
    "        for point in data:\n",
    "            isochrone_polygon.append(geom.Point(point[0], point[1]))\n",
    "        isochrone_polygon = geom.Polygon(isochrone_polygon)\n",
    "\n",
    "        return isochrone_polygon\n",
    "    except Exception as e:\n",
    "        logging.error(e)\n",
    "        logging.error(traceback.format_exc() )\n",
    "\n",
    "        raise Exception(\"Failed to fetch Isochrone Polygon\")\n",
    "\n",
    "\n",
    "def buffer_a_polygon_in_meters(polygon, meters):\n",
    "    import pyproj\n",
    "    from shapely.ops import transform\n",
    "    \n",
    "    wgs84 = pyproj.CRS('EPSG:4326')\n",
    "    utm = pyproj.CRS('EPSG:32618')\n",
    "    project_1 = pyproj.Transformer.from_crs(wgs84, utm, always_xy=True).transform\n",
    "    project_2 = pyproj.Transformer.from_crs(utm, wgs84, always_xy=True).transform\n",
    "\n",
    "    if type(polygon) == str:\n",
    "        polygon = wkt.loads(polygon)\n",
    "    if meters > 0:\n",
    "        polygon = transform(project_1, polygon)\n",
    "        polygon = polygon.buffer(meters)\n",
    "        polygon = transform(project_2, polygon)\n",
    "    return polygon\n",
    "\n",
    "def buffer_polygons(polygon,min,max, buffer_area_cutt_off):\n",
    "    if type(polygon) == str:\n",
    "        polygon = wkt.loads(polygon)\n",
    "    area = geom_area_in_sqkm(polygon)*1e6\n",
    "    if area < buffer_area_cutt_off:\n",
    "        amount_to_buffer = np.sqrt( (buffer_area_cutt_off - area) / (2*3.14)  )\n",
    "        amount_to_buffer = np.clip(amount_to_buffer, min, max)\n",
    "        polygon = buffer_a_polygon_in_meters(polygon, amount_to_buffer)\n",
    "    return polygon\n",
    "\n",
    "\n",
    "\n",
    "def get_num_optimal_clusters_for_a_city(bounds_polygon, base_zone_area_km_2=300):\n",
    "    area_bounds = geom_area_in_sqkm(bounds_polygon)\n",
    "    num_zones = area_bounds / base_zone_area_km_2\n",
    "    return int(num_zones)\n",
    "\n",
    "def get_optimal_r_in_meters_for_central_polygon(bounds_polygon, base_zone_area_km_2=300):\n",
    "    r = np.sqrt( (base_zone_area_km_2*1e6) / (2*3.14)  )\n",
    "    return r\n",
    "\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    # Haversine formula to calculate the distance between two points\n",
    "    R = 6371000  # Earth radius in meters\n",
    "    dlat = radians(lat2 - lat1)\n",
    "    dlon = radians(lon2 - lon1)\n",
    "    a = sin(dlat/2) * sin(dlat/2) + cos(radians(lat1)) * cos(radians(lat2)) * sin(dlon/2) * sin(dlon/2)\n",
    "    c = 2 * asin(sqrt(a))\n",
    "    distance = R * c\n",
    "    return distance\n",
    "\n",
    "def calculate_new_coordinates(lat, lon, distance, bearing):\n",
    "    # Calculate new coordinates based on distance and bearing\n",
    "    R = 6371000  # Earth radius in meters\n",
    "    lat_rad = radians(lat)\n",
    "    lon_rad = radians(lon)\n",
    "    angular_distance = distance / R\n",
    "    bearing_rad = radians(bearing)\n",
    "\n",
    "    new_lat_rad = asin(sin(lat_rad) * cos(angular_distance) + cos(lat_rad) * sin(angular_distance) * cos(bearing_rad))\n",
    "    new_lon_rad = lon_rad + atan2(sin(bearing_rad) * sin(angular_distance) * cos(lat_rad),\n",
    "                                  cos(angular_distance) - sin(lat_rad) * sin(new_lat_rad))\n",
    "\n",
    "    new_lat = degrees(new_lat_rad)\n",
    "    new_lon = degrees(new_lon_rad)\n",
    "\n",
    "    return new_lat, new_lon\n",
    "\n",
    "def get_points_around_center(center_lat, center_lon, distance):\n",
    "    # Calculate points in north, south, east, and west directions\n",
    "    north_west = calculate_new_coordinates(center_lat, center_lon, distance, 315)\n",
    "    south_east = calculate_new_coordinates(center_lat, center_lon, distance, 135)\n",
    "    north_east = calculate_new_coordinates(center_lat, center_lon, distance, 45)\n",
    "    south_west = calculate_new_coordinates(center_lat, center_lon, distance, 225)\n",
    "\n",
    "    return north_west, south_east, north_east, south_west\n",
    "\n",
    "\n",
    "\n",
    "def get_polygon_empty_areas_of_zones(all_zones_polygon, total_city_bouding_box_polygon):\n",
    "    empty_areas = total_city_bouding_box_polygon\n",
    "    for zone in all_zones_polygon:\n",
    "        empty_areas = empty_areas.difference(zone)\n",
    "    return empty_areas.geoms\n",
    "\n",
    "def voronoi_polygons(polygon, num_points):\n",
    "    # Generate random points within the bounding box of the polygon\n",
    "    min_x, min_y, max_x, max_y = polygon.bounds\n",
    "    points = np.random.uniform(low=min_x, high=max_x, size=(num_points, 2))\n",
    "\n",
    "    # Add the vertices of the polygon to the points\n",
    "    polygon_vertices = np.array(polygon.exterior.coords)\n",
    "    points = np.vstack([points, polygon_vertices])\n",
    "\n",
    "    # Create Voronoi diagram\n",
    "    vor = Voronoi(points)\n",
    "\n",
    "    # Create Voronoi polygons\n",
    "    polygons = []\n",
    "    for region in vor.regions:\n",
    "        if -1 not in region and len(region) > 0:\n",
    "            polygon_coords = [vor.vertices[i] for i in region]\n",
    "            polygons.append(Polygon(polygon_coords).intersection(polygon))\n",
    "\n",
    "    return polygons\n",
    "\n",
    "def generate_all_valid_vernoi_polygons_for_empty_geoms(empty_geoms, num_points=50):\n",
    "    all_vernoi_polygons = []\n",
    "    for empty_geom in empty_geoms:\n",
    "        curr_vernoi_polygons = voronoi_polygons(empty_geom, num_points)\n",
    "        all_vernoi_polygons.extend([ polygon for polygon in curr_vernoi_polygons if not polygon.is_empty ])\n",
    "    \n",
    "    return all_vernoi_polygons\n",
    "\n",
    "\n",
    "def add_a_polygon_to_a_zone(zones_geoms: Dict, valid_vernoi_polygons):\n",
    "    # check distance of each polygon center from each zone edge and add the polygon to the zone with minimum distance\n",
    "    for polygon in valid_vernoi_polygons:\n",
    "        min_d = 1e9\n",
    "        min_zone = None\n",
    "        zone_id_ = None\n",
    "        for zone_id,zone in zones_geoms.items():\n",
    "            if zone_id==4:\n",
    "                continue\n",
    "            d = polygon.centroid.distance(zone)\n",
    "            if d < min_d:\n",
    "                min_d = d\n",
    "                min_zone = zone\n",
    "                zone_id_ = zone_id\n",
    "        zones_geoms[zone_id_] = min_zone.union(polygon)\n",
    "    return zones_geoms\n",
    "\n",
    "def remove_clip_from_central_zone(id,geometry,central_geometry, central_zone_id):\n",
    "    if id != central_zone_id:\n",
    "        return geometry.difference(central_geometry)\n",
    "    else:\n",
    "        return geometry\n",
    "\n",
    "def prepare_zone_dict(zones_df):\n",
    "    data_d = zones_df[['cluster', 'geometry']].to_dict(orient='records')\n",
    "    zones_dict = {}\n",
    "    for d in data_d:\n",
    "        zones_dict[d['cluster']] = d['geometry']\n",
    "    \n",
    "    return zones_dict\n",
    "\n",
    "def get_points_from_polygon_multipolygon_geometry_collection(geometry_collection):\n",
    "    import shapely.geometry as geom\n",
    "    points = []\n",
    "    if type(geometry_collection) != geom.GeometryCollection and type(geometry_collection) != geom.MultiPolygon:\n",
    "        for k in  geometry_collection.exterior.coords:\n",
    "            points.append( (k[0], k[1]) )\n",
    "        return points\n",
    "    \n",
    "    for geo in geometry_collection.geoms:\n",
    "        if type(geo) == geom.Polygon:\n",
    "            for point in geo.exterior.coords:\n",
    "                points.append( (point[0], point[1]) )\n",
    "        elif type(geo) == geom.MultiPolygon:\n",
    "            for polygon in geo.geoms:\n",
    "                for point in polygon.exterior.coords:\n",
    "                    points.append( (point[0], point[1]) )\n",
    "    return points\n",
    "\n",
    "def concave_hull(polygon):\n",
    "    points = get_points_from_polygon_multipolygon_geometry_collection(polygon)\n",
    "    \n",
    "    alpha_shape = alphashape.alphashape(points, 0.001)\n",
    "    return alpha_shape\n",
    "\n",
    "def get_concat_hull_for_zones(zones_dict):\n",
    "    for k,v in zones_dict.items():\n",
    "        zones_dict[k] = concave_hull(v)\n",
    "    return zones_dict\n",
    "\n",
    "def subtract_each_polygon_from_another(return_zones):\n",
    "    for i in range(len(return_zones)):\n",
    "        if i == 4:\n",
    "            continue \n",
    "        \n",
    "        for j in range(len(return_zones)):\n",
    "            if i != j:\n",
    "                return_zones[i] = return_zones[i].difference(return_zones[j])\n",
    "    return return_zones\n",
    "\n",
    "\n",
    "def quadkey_to_wkt(qk):\n",
    "    qk = quadkey.QuadKey(qk)\n",
    "    ne = qk.to_geo(quadkey.TileAnchor.ANCHOR_NE)\n",
    "    sw = qk.to_geo(quadkey.TileAnchor.ANCHOR_SW)\n",
    "    nw = qk.to_geo(quadkey.TileAnchor.ANCHOR_NW)\n",
    "    se = qk.to_geo(quadkey.TileAnchor.ANCHOR_SE)\n",
    "    \n",
    "    return geom.box(nw[1], nw[0], se[1], se[0])\n",
    "\n",
    "\n",
    "\n",
    "def n_lenght_all_combi(n):\n",
    "    available_chars = ['0', '1', '2', '3']\n",
    "    return [\"\".join(i) for i in list(itertools.product(available_chars, repeat=n))]\n",
    "\n",
    "def generate_n_lvl_neighbour_quadkeys(quadkey, n):\n",
    "    # n = 0 means the same quadkey\n",
    "    # n= 1 means the 8 neighbours of the quadkey and qaudkey itself\n",
    "    # n = 2 means the 8 neighbours of the quadkey and qaudkey itself and the 8 neighbours of the neighbours\n",
    "    \n",
    "    current_lvl = len(quadkey)\n",
    "    if current_lvl < n + 1:\n",
    "        return []\n",
    "    \n",
    "    if n==0:\n",
    "        return [quadkey]\n",
    "    \n",
    "    lvls_to_skip = quadkey[:-n]\n",
    "    all_neighbours_lvl_n = n_lenght_all_combi(n)\n",
    "    return [lvls_to_skip + i for i in all_neighbours_lvl_n]\n",
    "\n",
    "def get_level_for_n_grids(n_grids):\n",
    "    return math.ceil(math.log(n_grids, 4))\n",
    "\n",
    "def total_fn(series_custome_series):\n",
    "    return len( [ i for i in series_custome_series if i[0] == 1 ] )\n",
    "\n",
    "def branded_fn(series_custome_series):\n",
    "    return len( [ i for i in series_custome_series if i[1] is not None and i[1] != 'N_A' ] )\n",
    "\n",
    "def verified_fn(series_custome_series):\n",
    "    return len( [ i for i in series_custome_series if i[2] in [1,2,4] ] )\n",
    "\n",
    "def gt_50votes_fn(series_custome_series):\n",
    "    return len( [ i for i in series_custome_series if i[3] >= 50 ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def fetch_city_counts_for_competitors(city_id , competitor_brand_ids=[], anchor_brand_ids=[]):\n",
    "    competitor_brand_ids = competitor_brand_ids + [str(random.randint(10000,100000) ),str(random.randint(10000,100000) ) ] if len(competitor_brand_ids) <2 else competitor_brand_ids\n",
    "    anchor_brand_ids = anchor_brand_ids + [str(random.randint(10000,100000) ),str(random.randint(10000,100000) ) ] if len(anchor_brand_ids) <2 else anchor_brand_ids\n",
    "    \n",
    "    city_query = f\"\"\"\n",
    "    select \n",
    "        lvl_16_quadkey\n",
    "        , category\n",
    "        , city_id\n",
    "        , city_name,\n",
    "        0 as branded ,\n",
    "        0 as verified,\n",
    "        count(*) as total,\n",
    "        0 as high_voted_gt_50\n",
    "    from \n",
    "    (\n",
    "        select \n",
    "            case when A.brand_id in {tuple(competitor_brand_ids)} then 'competitor'\n",
    "            when A.brand_id in {tuple(anchor_brand_ids)} then 'anchor' \n",
    "            else 'other' end as category,\n",
    "            B.id as city_id,\n",
    "            B.name as city_name,\n",
    "            bing_tile_quadkey(bing_tile_at(A.lat, A.lng, 16)) lvl_16_quadkey\n",
    "        from datasets_prep.ind_poi_data_v2_gold A \n",
    "        cross join (\n",
    "            select *\n",
    "            from hyperlocal_analysis_ind_dev.ind_top_8_cities_geometry\n",
    "            where id = {city_id}\n",
    "        ) B\n",
    "        where st_intersects(\n",
    "                st_point(A.lng, A.lat),\n",
    "                st_geometryfromtext(B.geometry)\n",
    "            )\n",
    "        and A.brand_id in {tuple(competitor_brand_ids + anchor_brand_ids)}\n",
    "        and A.active = 1\n",
    "        )\n",
    "        group by lvl_16_quadkey,\n",
    "            category,\n",
    "            city_id,\n",
    "            city_name\n",
    "    \"\"\"\n",
    "    data = wr.athena.read_sql_query(city_query, database='hyperlocal_analysis_ind_dev', ctas_approach=False)\n",
    "    return data\n",
    "\n",
    "def fetch_isochrone_counts_for_competitors(isochrone_polygon, competitor_brand_ids=[], anchor_brand_ids=[], ):\n",
    "    competitor_brand_ids = competitor_brand_ids + [str(random.randint(10000,100000) ),str(random.randint(10000,100000) ) ] if len(competitor_brand_ids) <2 else competitor_brand_ids\n",
    "    anchor_brand_ids = anchor_brand_ids + [str(random.randint(10000,100000) ),str(random.randint(10000,100000) ) ] if len(anchor_brand_ids) <2 else anchor_brand_ids\n",
    "    \n",
    "    isochrone_query = f\"\"\"\n",
    "    select \n",
    "        category,\n",
    "        0 as branded ,\n",
    "        0 as verified,\n",
    "        count(*) as total,\n",
    "        0 as high_voted_gt_50\n",
    "    from\n",
    "    (\n",
    "    select\n",
    "        case when A.brand_id in {tuple(competitor_brand_ids)} then 'competitor'\n",
    "            when A.brand_id in {tuple(anchor_brand_ids)} then 'anchor' \n",
    "            else 'other' end as category\n",
    "        from datasets_prep.ind_poi_data_v2_gold A \n",
    "        where st_intersects(\n",
    "                st_point(A.lng, A.lat),\n",
    "                st_geometryfromtext('\"\"\" + isochrone_polygon.wkt + f\"\"\"')\n",
    "            )\n",
    "            and A.brand_id in {tuple(competitor_brand_ids + anchor_brand_ids)}\n",
    "            and A.active = 1\n",
    "        ) group by category\n",
    "    \"\"\"\n",
    "    data = wr.athena.read_sql_query(isochrone_query, database='hyperlocal_analysis_ind_dev', ctas_approach=False)\n",
    "    return data\n",
    "\n",
    "def get_categories_and_count_type_needed():\n",
    "    category_wise_count_needed = {\n",
    "        \"vibrancy\" : {\n",
    "            \"restaurant\" : 'branded',\n",
    "            \"bar_pub\" : 'branded',\n",
    "            'food_other' : 'verified',\n",
    "            'coffee_shop' : 'verified',\n",
    "        },\n",
    "        'fashion_index': {\n",
    "            'clothing_store' : 'verified',\n",
    "            'shoe_store' : 'verified',\n",
    "            'jewelry_store' : 'verified',\n",
    "            'cosmetic' : 'verified',\n",
    "            'salon' : 'verified',\n",
    "        },\n",
    "        'healthcare_index': {\n",
    "        'hospital' : 'branded',\n",
    "            'pharmacy' : 'branded',\n",
    "            'clinic': 'branded',\n",
    "        },\n",
    "        'malls_index': {\n",
    "            'shopping_mall' : 'verified',\n",
    "        },\n",
    "        'education_index': {\n",
    "            'school' : 'verified',\n",
    "            'college': 'verified',\n",
    "            'tuition_center' : 'verified',\n",
    "        },\n",
    "        'public_transport_index': {\n",
    "            'bus_stop' : 'total',\n",
    "            'metro_station' : 'total',\n",
    "        },\n",
    "        'entertainment_index': {\n",
    "            'cinema_hall' : 'verified',\n",
    "        },\n",
    "        'leisure_index': {\n",
    "            'gym_fitness' : 'verified',\n",
    "            'tourist_attraction' : 'verified',\n",
    "        },\n",
    "        'grocery_index': {\n",
    "            \"grocery_store\" : 'verified',\n",
    "        },\n",
    "        'supermarket_index': {\n",
    "            \"supermarket\" : 'verified',\n",
    "        },\n",
    "        'religious_index': {\n",
    "            \"religious_place\" : 'verified',\n",
    "        },\n",
    "        'company_index': {\n",
    "            'Private Sector' : 'total',\n",
    "            'Public Sector' : 'total',\n",
    "            'Govt Sector' : 'total',\n",
    "            'office' : 'verified',\n",
    "        },\n",
    "        'electronics_index': {\n",
    "            'electronic_store' : 'verified',\n",
    "        },\n",
    "        'home_decor_index': {\n",
    "            'home_decor' : 'verified',\n",
    "        },\n",
    "        'parks_index': {\n",
    "            'park' : 'total',\n",
    "        },\n",
    "        'connectivity_index':{\n",
    "            'road_area': 'total'\n",
    "        },\n",
    "        'apartments_index':{\n",
    "            'apartments' : 'total'\n",
    "        },\n",
    "        'affluence_index':{\n",
    "            'affluence' : 'total'\n",
    "        },\n",
    "        'competitor':{\n",
    "            'competitor' : 'total'\n",
    "        },\n",
    "        'anchor':{\n",
    "            'anchor' : 'total'\n",
    "        }\n",
    "    }\n",
    "\n",
    "    all_category_wise_count_needed_pair = {}\n",
    "    for k,v in category_wise_count_needed.items():\n",
    "        for k1,v1 in v.items():\n",
    "            all_category_wise_count_needed_pair[k1] = v1\n",
    "    \n",
    "    return all_category_wise_count_needed_pair\n",
    "\n",
    "def get_city_grid_level_counts(city_id, level_to_get_neighbours,quantile=0.95, competitor_ids = [], anchor_id=[]):\n",
    "    base_count_data = wr.athena.read_sql_query(\n",
    "        \"\"\" SELECT * FROM \"hyperlocal_analysis_ind_dev\".\"ind_poi_counts_by_city_quadkeys\" where city_id={} \"\"\".format(city_id),\n",
    "        database='hyperlocal_analysis_ind_dev',\n",
    "    )\n",
    "    parks_data_city_query = \"\"\" select \n",
    "        lvl_16_quadkey\n",
    "        , category\n",
    "        , city_id\n",
    "        , city_name,\n",
    "        0 as branded ,\n",
    "        0 as verified,\n",
    "        sum(area)/2e5 as total,\n",
    "        0 as high_voted_gt_50\n",
    "        from (\n",
    "                select A.place_id as id,\n",
    "                    A.lat,\n",
    "                    A.lng,\n",
    "                    A.park_polygon,\n",
    "                    case\n",
    "                        when A.area > 0 then A.area else 7755\n",
    "                    end as area,\n",
    "                    'park' as category,\n",
    "                    B.id as city_id,\n",
    "                    B.name as city_name,\n",
    "                    bing_tile_quadkey(bing_tile_at(A.lat, A.lng, 16)) lvl_16_quadkey\n",
    "                from datasets_prep.parks A\n",
    "                    cross join (\n",
    "                        select *\n",
    "                        from hyperlocal_analysis_ind_dev.ind_top_8_cities_geometry\n",
    "                        where id = 3\n",
    "                    ) B\n",
    "                where st_intersects(\n",
    "                        st_point(A.lng, A.lat),\n",
    "                        st_geometryfromtext(B.geometry)\n",
    "                    )\n",
    "                    and active = 1\n",
    "            )\n",
    "        group by lvl_16_quadkey,\n",
    "            category,\n",
    "            city_id,\n",
    "            city_name \"\"\"\n",
    "    \n",
    "    apparments_query = \"\"\"  select \n",
    "        lvl_16_quadkey\n",
    "        , category\n",
    "        , city_id\n",
    "        , city_name,\n",
    "        0 as branded ,\n",
    "        0 as verified,\n",
    "        sum(default_units) / 2e3 as total,\n",
    "        0 as high_voted_gt_50\n",
    "        from (\n",
    "                select A.id as id,\n",
    "                    A.lat,\n",
    "                    A.lng,\n",
    "                    A.default_units,\n",
    "                    'apartments' as category,\n",
    "                    B.id as city_id,\n",
    "                    B.name as city_name,\n",
    "                    bing_tile_quadkey(bing_tile_at(A.lat, A.lng, 16)) lvl_16_quadkey\n",
    "                from datasets_prep.bng_residential_projects A\n",
    "                    cross join (\n",
    "                        select *\n",
    "                        from hyperlocal_analysis_ind_dev.ind_top_8_cities_geometry\n",
    "                        where id = 3\n",
    "                    ) B\n",
    "                where st_intersects(\n",
    "                        st_point(A.lng, A.lat),\n",
    "                        st_geometryfromtext(B.geometry)\n",
    "                    )\n",
    "            )\n",
    "        group by lvl_16_quadkey,\n",
    "            category,\n",
    "            city_id,\n",
    "            city_name \"\"\"\n",
    "            \n",
    "    base_count_data = base_count_data[base_count_data['category'] != 'park']\n",
    "    parks_count_data = wr.athena.read_sql_query(parks_data_city_query, database='hyperlocal_analysis_ind_dev',)\n",
    "    apparments_count_data = wr.athena.read_sql_query(apparments_query, database='hyperlocal_analysis_ind_dev',)\n",
    "    competitor_anchors = fetch_city_counts_for_competitors(city_id, competitor_ids, anchor_id)\n",
    "    \n",
    "    base_count_data = base_count_data.append(parks_count_data).append(apparments_count_data).append(competitor_anchors)\n",
    "    \n",
    "    all_category_wise_count_needed_pair = get_categories_and_count_type_needed()\n",
    "    base_count_data['count_needed'] = base_count_data[['category', 'branded', 'verified', 'total']].apply(lambda x: all_category_wise_count_needed_pair[x['category']] if x['category'] in all_category_wise_count_needed_pair.keys() else None, axis=1)\n",
    "    base_count_data['count'] = base_count_data[['category', 'branded', 'verified', 'total', 'count_needed']].apply(lambda x: x[x['count_needed']] if x['count_needed'] is not None else None, axis=1)\n",
    "    base_count_data_filtered = base_count_data[base_count_data['count_needed'].notnull()][[ 'lvl_16_quadkey','category', 'count',]]\n",
    "    base_count_data_filtered = base_count_data_filtered.pivot(index='lvl_16_quadkey', columns='category', values='count').reset_index().fillna(0)\n",
    "        \n",
    "    neighbour_df = base_count_data_filtered[['lvl_16_quadkey']]\n",
    "    neighbour_df['level_neighbours'] = neighbour_df['lvl_16_quadkey'].apply(lambda x: generate_n_lvl_neighbour_quadkeys(x, level_to_get_neighbours))\n",
    "    neighbour_df = neighbour_df.explode('level_neighbours')\n",
    "    joined = neighbour_df.merge(base_count_data_filtered, left_on='level_neighbours', right_on='lvl_16_quadkey', how='left')\n",
    "    joined = joined[joined['lvl_16_quadkey_y'].notnull()]\n",
    "\n",
    "    agg_dict = { i:'sum' for i in joined.columns if i not in ['lvl_16_quadkey_x', 'level_neighbours', 'lvl_16_quadkey_y' ] }\n",
    "    agg_dict['lvl_16_quadkey_y'] = 'count'\n",
    "    \n",
    "    city_quantiles = joined.groupby('lvl_16_quadkey_x').agg(agg_dict).reset_index().quantile(quantile).to_dict()\n",
    "    \n",
    "    ## add road_area_quantile \n",
    "    city_quantiles['road_area'] = 0.30 * 50\n",
    "    city_quantiles['affluence'] = 3.84\n",
    "    return city_quantiles\n",
    "\n",
    "def pois_count_for_isochrone(isochrone_polygon,competitor_ids=[],anchor_ids=[]):\n",
    "    query_for_pois_in_isochrone = \"\"\" select category\n",
    "    , sum((CASE WHEN ((brand_id IS NOT NULL) AND (brand_id <> 'N_A')) THEN 1 ELSE 0 END)) branded\n",
    "    , sum((CASE WHEN (verified IN (1, 2, 4)) THEN 1 ELSE 0 END)) verified\n",
    "    , count(id) total\n",
    "    , sum((CASE WHEN (number_of_votes > 50) THEN 1 ELSE 0 END)) high_voted_gt_50\n",
    "    from (\n",
    "            select *\n",
    "            from hyperlocal_analysis_ind_dev.ind_poi_data_v2\n",
    "            where st_intersects(\n",
    "                    st_point(lng, lat),\n",
    "                    st_geometryfromtext('\"\"\" + isochrone_polygon.wkt + \"\"\"')\n",
    "        ) ) group by category\"\"\"\n",
    "    \n",
    "    parks_data_isochrone_query = \"\"\" select \n",
    "    'park' as category,\n",
    "    0 as branded ,\n",
    "    0 as verified,\n",
    "        sum(area)/2e5 as total,\n",
    "        0 as high_voted_gt_50\n",
    "    from (\n",
    "        select * from datasets_prep.parks A\n",
    "        where st_intersects(\n",
    "                st_point(A.lng, A.lat),\n",
    "                st_geometryfromtext('\"\"\" + isochrone_polygon.wkt + \"\"\"')\n",
    "            )\n",
    "            and active = 1\n",
    "        ) \"\"\"\n",
    "        \n",
    "    road_area_grided_query = \"\"\" select\n",
    "        'road_area' as category,\n",
    "        0 as branded ,\n",
    "        0 as verified,\n",
    "        (sum(road_area) * 50) / sum(total_area) as total,\n",
    "        0 as high_voted_gt_50\n",
    "        from \n",
    "        \"hyperlocal_analysis_ind_dev\".\"ind_road_covered_area_gridded\" A \n",
    "        where st_intersects(\n",
    "            st_geometryfromtext('\"\"\" + isochrone_polygon.wkt + \"\"\"'),\n",
    "            st_geometryfromtext(A.polygon)\n",
    "            )\n",
    "    \"\"\" \n",
    "    \n",
    "    aparments_data_isochrone_query = \"\"\" select \n",
    "    'apartments' as category,\n",
    "    0 as branded ,\n",
    "    0 as verified,\n",
    "        sum(default_units)/2e3 as total,\n",
    "        0 as high_voted_gt_50\n",
    "    from (\n",
    "        select * from datasets_prep.bng_residential_projects A\n",
    "        where st_intersects(\n",
    "                st_point(A.lng, A.lat),\n",
    "                st_geometryfromtext('\"\"\" + isochrone_polygon.wkt + \"\"\"')\n",
    "            )\n",
    "        ) \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    affluence_query = \"\"\" \n",
    "    select \n",
    "    'affluence' as category,\n",
    "    0 as branded ,\n",
    "    0 as verified,\n",
    "        (avg(affluence_index)*4)/5 as total,\n",
    "        0 as high_voted_gt_50\n",
    "    from (\n",
    "        select affluence_index from \"hyperlocal_analysis_ind_dev\".\"ind_geoid_indices\" A\n",
    "        where st_intersects(\n",
    "               st_geometryfromtext(geometry),\n",
    "                st_geometryfromtext('\"\"\" + isochrone_polygon.wkt + \"\"\"')\n",
    "            )\n",
    "            and type = '1km_grid'\n",
    "        )  \"\"\"\n",
    "    \n",
    "    all_pois_counts = wr.athena.read_sql_query(query_for_pois_in_isochrone, database='hyperlocal_analysis_ind_dev', ctas_approach=False)\n",
    "    all_pois_counts = all_pois_counts[all_pois_counts['category'] != 'park']\n",
    "    park_counts = wr.athena.read_sql_query(parks_data_isochrone_query, database='hyperlocal_analysis_ind_dev', ctas_approach=False)\n",
    "    road_area_counts = wr.athena.read_sql_query(road_area_grided_query, database='hyperlocal_analysis_ind_dev', ctas_approach=False)\n",
    "    aparments_counts = wr.athena.read_sql_query(aparments_data_isochrone_query, database='hyperlocal_analysis_ind_dev', ctas_approach=False)\n",
    "    affluence_query = wr.athena.read_sql_query(affluence_query, database='hyperlocal_analysis_ind_dev', ctas_approach=False)\n",
    "    competitor_anchors = fetch_isochrone_counts_for_competitors(isochrone_polygon, competitor_ids, anchor_ids)\n",
    "    \n",
    "    all_pois_counts = all_pois_counts.append(park_counts).append(road_area_counts).append(aparments_counts).append(affluence_query).append(competitor_anchors)\n",
    "    \n",
    "    all_category_wise_count_needed_pair = get_categories_and_count_type_needed()\n",
    "    all_pois_counts['count_needed'] = all_pois_counts[['category', 'branded', 'verified', 'total']].apply(lambda x: all_category_wise_count_needed_pair[x['category']] if x['category'] in all_category_wise_count_needed_pair.keys() else None, axis=1)\n",
    "    all_pois_counts['count'] = all_pois_counts[['category', 'branded', 'verified', 'total', 'count_needed']].apply(lambda x: x[x['count_needed']] if x['count_needed'] is not None else None, axis=1)\n",
    "    all_pois_counts = all_pois_counts[all_pois_counts['count_needed'].notnull()][[ 'category', 'count',]]\n",
    "    all_pois_counts = all_pois_counts.set_index('category').to_dict()['count']\n",
    "    return all_pois_counts\n",
    "\n",
    "def create_index_from_city_isochrone_counts(isochrone_counts, city_counts, isochrone_area, city_grid_level, total_area_at_this_level):\n",
    "    category_indexs = {}\n",
    "    for key in city_counts:\n",
    "        if key not in isochrone_counts.keys():\n",
    "            continue\n",
    "        city_val = city_counts[key] *  isochrone_area / total_area_at_this_level if key not in [ 'road_area', 'affluence' ] else city_counts[key]\n",
    "        isochrone_val = isochrone_counts[key]\n",
    "        if int(city_val) in [0,1] and int(isochrone_val) ==0 :\n",
    "            category_indexs[key] = 0 \n",
    "            continue\n",
    "        if isochrone_val <= 1 or city_val <= 1:\n",
    "            isochrone_val += 2\n",
    "            city_val += 2\n",
    "            \n",
    "        if isochrone_val > city_val:\n",
    "            category_indexs[key] = 5\n",
    "        else:\n",
    "            category_indexs[key] = round( (100 * (np.log(isochrone_val) / np.log(city_val) ) ) / 20, 2)\n",
    "    \n",
    "    return category_indexs\n",
    "\n",
    "def load_city_count(city_id,level,quantile=0.99, competitor_ids = [], anchor_id=[]):\n",
    "    base_path = \"/mnt/ssd2_space2/projects/python/tuzo/additional_tasks_data/revenue_score/cache_data_for_city_grids/\"\n",
    "    if os.path.exists(f\"{base_path}city_count_{city_id}_{level}.pkl\"):\n",
    "        city_count = pickle.load(open(f\"{base_path}city_count_{city_id}_{level}.pkl\", \"rb\"))\n",
    "    else:\n",
    "        city_count = get_city_grid_level_counts(city_id, level, quantile, competitor_ids, anchor_id)\n",
    "        pickle.dump(city_count, open(f\"{base_path}city_count_{city_id}_{level}.pkl\", \"wb\"))\n",
    "    \n",
    "    return city_count, 0.35 * 4**level\n",
    "\n",
    "def get_isochrone_and_area(lat,lng,cost,type='driving', cost_type='time'):\n",
    "    isochrone_polygon = get_isochrone(lat,lng,type,cost_type,cost)\n",
    "    return isochrone_polygon, geom_area_in_sqkm(isochrone_polygon)\n",
    "\n",
    "\n",
    "def create_grouped_indexes_from_indexs(indexs):\n",
    "    category_wise_count_needed_weighted = {\n",
    "        \"vibrancy\" : {\n",
    "            \"restaurant\" : 0.5,\n",
    "            \"bar_pub\" : 0.2,\n",
    "            'food_other' : 0,\n",
    "            'coffee_shop' : 0.3\n",
    "            \n",
    "        },\n",
    "        'fashion_index': {\n",
    "            'clothing_store' : 0.4,\n",
    "            'shoe_store' : 0.3,\n",
    "            'jewelry_store' : 0.2,\n",
    "            'cosmetic' : 0.1,\n",
    "            'salon' : 0.0\n",
    "        },\n",
    "        \"healthcare_index\": {\n",
    "            'hospital' : 0.5,\n",
    "            'pharmacy' : 0.2,\n",
    "            'clinic': 0.3\n",
    "        },\n",
    "        'malls_index': {\n",
    "            'shopping_mall' : 1.0\n",
    "        },\n",
    "        'education_index': {\n",
    "            'school' : 0.5,\n",
    "            'college': 0.4,\n",
    "            'tuition_center' : 0.1\n",
    "        },\n",
    "        'connectivity_index': {\n",
    "            \"bus_stop\" : 0.3,\n",
    "            \"metro_station\" : 0.2,\n",
    "            'road_area': 0.5\n",
    "        },\n",
    "        'entertainment_index': {\n",
    "            'cinema_hall' : 1.0\n",
    "        },\n",
    "        'leisure_index': {\n",
    "            'gym_fitness' : 0.7,\n",
    "            'tourist_attraction' : 0.3\n",
    "        },\n",
    "        'grocery_index': {\n",
    "            \"grocery_store\" : 1.0\n",
    "        },\n",
    "        'supermarket_index': {\n",
    "            \"supermarket\" : 1.0\n",
    "        },\n",
    "        'religious_index': {\n",
    "            \"religious_place\" : 1.0\n",
    "        },\n",
    "        'company_index': {\n",
    "            'Private Sector' : 0.7,\n",
    "            'Public Sector' : 0.1,\n",
    "            'Govt Sector' : 0.1,\n",
    "            'office' : 0.1\n",
    "        },\n",
    "        'electronics_index': {\n",
    "            'electronic_store' : 1.0\n",
    "        },\n",
    "        'home_decor_index': {\n",
    "            'home_decor' : 1.0\n",
    "        },\n",
    "        'parks_index': {\n",
    "            'park' : 1.0\n",
    "        },\n",
    "        'apartments_index': {\n",
    "            'apartments' : 1.0\n",
    "        },\n",
    "        'affluence_index': {\n",
    "            'affluence' : 1.0\n",
    "        },\n",
    "        'competitor_index':{\n",
    "            'competitor' : 0.6,\n",
    "            'anchor' : 0.4\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    all_category_wise_count_needed_pair = []\n",
    "    for key, value in category_wise_count_needed_weighted.items():\n",
    "        for k,v in value.items():\n",
    "            all_category_wise_count_needed_pair.append((key, k, v, indexs.get(k,-1)))\n",
    "    all_category_wise_count_needed_pair_df = pd.DataFrame(all_category_wise_count_needed_pair, columns=['index_type', 'category', 'weight', 'index'])\n",
    "    all_category_wise_count_needed_pair_df['weighted_index'] = all_category_wise_count_needed_pair_df['index'] * all_category_wise_count_needed_pair_df['weight']\n",
    "    \n",
    "    all_category_wise_count_needed_pair_df = all_category_wise_count_needed_pair_df[all_category_wise_count_needed_pair_df['index'] != -1]\n",
    "    all_category_wise_count_needed_pair_df = all_category_wise_count_needed_pair_df.groupby('index_type').agg({'weighted_index':'sum', 'weight':'sum'}).reset_index()\n",
    "    all_category_wise_count_needed_pair_df['index'] = all_category_wise_count_needed_pair_df['weighted_index'] / all_category_wise_count_needed_pair_df['weight']\n",
    "    return all_category_wise_count_needed_pair_df[['index_type', 'index']].round(2).set_index('index_type').to_dict()['index']\n",
    "\n",
    "def create_revenue_score_from_indexs(grouped_indexs, weights_dict):\n",
    "    score = 0\n",
    "    for k,v in grouped_indexs.items():\n",
    "        if k in weights_dict.keys():\n",
    "            score += v * weights_dict[k]\n",
    "    return score / sum(weights_dict.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "competitor_brand_ids=['jockeyindia', 'zivame', 'nykdbynykaa', 'enamor', 'adityabirla', 'vanheusenindia','jockey']\n",
    "anchor_brand_ids=['blown', 'bouncehere', 'bblunt', 'nailashes', 'nykdbynykaa', 'sugarcosmetics', 'globaldesi','naturals', 'shopforaurelia', 'biba', 'yesmadam', 'rangriti', 'nykaa', 'nailbox','bibaindia', 'fabindia']\n",
    "    \n",
    "isochrone_polygon,isochrone_area = get_isochrone_and_area(\t12.912364446792184, 77.63807573906557, 15)\n",
    "\n",
    "grid_level_to_fetch = get_level_for_n_grids(isochrone_area/0.35)\n",
    "city_counts,total_area_at_this_level = load_city_count(3, grid_level_to_fetch, 0.95, competitor_brand_ids, anchor_brand_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import awswrangler as wr\\nimport pandas as pd\\nimport pickle\\nfrom shapely import geometry as geom\\nimport os'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import awswrangler as wr\\nimport pandas as pd\\nimport pickle\\nfrom shapely import geometry as geom\\nimport os'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import awswrangler as wr\\nimport pandas as pd\\nimport pickle\\nfrom shapely import geometry as geom\\nimport os'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import awswrangler as wr\\nimport pandas as pd\\nimport pickle\\nfrom shapely import geometry as geom\\nimport os'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import awswrangler as wr\\nimport pandas as pd\\nimport pickle\\nfrom shapely import geometry as geom\\nimport os'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import awswrangler as wr\\nimport pandas as pd\\nimport pickle\\nfrom shapely import geometry as geom\\nimport os'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "poi_counts_isochrone = pois_count_for_isochrone(isochrone_polygon, competitor_brand_ids, anchor_brand_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexs = create_index_from_city_isochrone_counts(poi_counts_isochrone, city_counts, isochrone_area, grid_level_to_fetch, total_area_at_this_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import awswrangler as wr\\nimport pandas as pd\\nimport pickle\\nfrom shapely import geometry as geom\\nimport os'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import awswrangler as wr\\nimport pandas as pd\\nimport pickle\\nfrom shapely import geometry as geom\\nimport os'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import awswrangler as wr\\nimport pandas as pd\\nimport pickle\\nfrom shapely import geometry as geom\\nimport os'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "indexs_df = pd.DataFrame.from_dict(indexs, orient='index').reset_index()\n",
    "indexs_df.columns = ['category', 'index']\n",
    "\n",
    "isochrone_poi_counts_df = pd.DataFrame.from_dict(poi_counts_isochrone, orient='index').reset_index()\n",
    "isochrone_poi_counts_df.columns = ['category', 'isochrone_counts']\n",
    "\n",
    "city_poi_counts_df = pd.DataFrame.from_dict(city_counts, orient='index').reset_index()\n",
    "city_poi_counts_df.columns = ['category', 'city_counts']\n",
    "\n",
    "all_merged = indexs_df.merge(isochrone_poi_counts_df, on='category', how='left').merge(city_poi_counts_df, on='category', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import awswrangler as wr\\nimport pandas as pd\\nimport pickle\\nfrom shapely import geometry as geom\\nimport os'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "94.24333333333334"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_indexs = create_grouped_indexes_from_indexs(indexs)\n",
    "\n",
    "weights_dict = {\n",
    " 'competitor_index': 0.20,\n",
    " 'affluence_index': 0.19,\n",
    " 'apartments_index': 0.19,\n",
    " 'fashion_index': 0.19,\n",
    " 'vibrancy': 0.14,\n",
    " 'healthcare_index': 0.10,\n",
    " 'company_index': 0.10,\n",
    " 'malls_index': 0.05,\n",
    " 'supermarket_index': 0.04,\n",
    " \n",
    " }\n",
    "\n",
    "revenue_score = create_revenue_score_from_indexs(grouped_indexs, weights_dict) * 20\n",
    "revenue_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'affluence_index': 3.79,\n",
       " 'apartments_index': 5.0,\n",
       " 'company_index': 4.02,\n",
       " 'competitor_index': 5.0,\n",
       " 'connectivity_index': 4.32,\n",
       " 'education_index': 5.0,\n",
       " 'electronics_index': 5.0,\n",
       " 'entertainment_index': 3.82,\n",
       " 'fashion_index': 4.95,\n",
       " 'grocery_index': 4.99,\n",
       " 'healthcare_index': 4.92,\n",
       " 'home_decor_index': 5.0,\n",
       " 'leisure_index': 4.65,\n",
       " 'malls_index': 5.0,\n",
       " 'parks_index': 3.96,\n",
       " 'religious_index': 0.0,\n",
       " 'supermarket_index': 5.0,\n",
       " 'vibrancy': 5.0}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_indexs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
